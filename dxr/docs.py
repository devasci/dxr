'''
This module implements documentation coverage for an entire codebase.

Documentation is generated by the indexers and added into the master
database. As there is no need to search documentation, an auxiliary file is
used to store the documentation data.

Four types of documentable entities are recognized by DXR. These correspond
to files, namespaces, aggregates, and leaves.

This module expects to be doing the documentation comment to HTML conversion
itself, using a syntax rather similar to that of Javadoc and Doxygen.
'''

import json
import os
import re

def escape_html(string):
    string = string.replace('&', '&amp;')
    string = string.replace('<', '&lt;')
    string = string.replace('>', '&gt;')
    return string

class DocumentableEntity(object):
  def __init__(self, jsondata, docid, lazyloader):
    self.docid = docid
    self._lazyloader = lazyloader
    for key, value in jsondata.iteritems():
      setattr(self, key, value)
    self._resolvedMembers = False

  def get_members(self, includeall):
    if not self._resolvedMembers:
      self.members = [self._lazyloader.getEntity(n) for n in self.members]
      self._resolvedMembers = True
    if includeall:
      return self.members
    else:
        return filter(lambda x: isinstance(x, LeafEntity), self.members)


  def getLocationString(self, wwwroot, tree):
    locs = self.location.split(":")
    filename = locs[0]
    line = locs[1]
    return ("Defined at <tt><a href=\"%(wwwroot)s/%(tree)s/" +
        "%(filename)s#l%(line)s\">%(location)s</a></tt>") % {
        "filename": filename,
        "wwwroot": wwwroot,
        "tree": tree,
        "line": line,
        "location": self.location}

  def getBriefDocumentation(self):
    return self.briefdoc

  def getFullDocumentation(self):
    return self.fulldoc

  def skip_prolog(self):
    return False

class FileEntity(DocumentableEntity):
    def __init__(self, jsondata, docid, lazyloader):
        DocumentableEntity.__init__(self, jsondata, docid, lazyloader)
        self.filename = self.location.split(':')[0]

    def getTitle(self):
        return os.path.basename(self.filename)

    def getHeader(self):
        return "file " + self.filename

    def skip_prolog(self):
        return True

class NamespaceEntity(DocumentableEntity):
  def __init__(self, jsondata, docid, lazyloader):
    DocumentableEntity.__init__(self, jsondata, docid, lazyloader)
    self.qualifiedName = jsondata['qualname']

  def getTitle(self):
    return self.qualifiedName

  def getHeader(self):
    return "namespace " + self.qualifiedName

  def skip_prolog(self):
    return True

  def getSummaryLink(self):
    return str(self.docid)

  def getPrename(self):
    return "namespace"

  def getPostname(self):
    return ""

class AggregateEntity(DocumentableEntity):
  def __init__(self, jsondata, docid, lazyloader):
    DocumentableEntity.__init__(self, jsondata, docid, lazyloader)
    self.qualifiedName = jsondata['qualname']

  def getTitle(self):
    return self.qualifiedName

  def getHeader(self):
    return self.kind + " " + self.qualifiedName

  def getProlog(self):
    return self.prolog

  def getPrename(self):
    return self.prename

  def getPostname(self):
    return ""

  def getSummaryLink(self):
    return str(self.docid)

class LeafEntity(DocumentableEntity):
    def __init__(self, jsondata, docid, lazyloader):
        DocumentableEntity.__init__(self, jsondata, docid, lazyloader)

    def getPrename(self):
        return self.prename
    def getPostname(self):
        return self.postname
    def getSummaryLink(self):
        return "#member%d" % self.docid # XXX


class DocumentationBuilder(object):
    def __init__(self, tree):
        self.tree = tree
        self.allEntities = list()
        self.qualmap = dict()

    def add_docs_from_tree(self, rootdir):
        ''' Find all *.json files under rootdir and add them to the database. '''
        for dirpath, dirs, files in os.walk(rootdir):
            for f in files:
                if f.endswith('.json'):
                    self.add_docs_from_file(os.path.join(dirpath, f))

    def add_docs_from_file(self, filename):
        ''' Open the JSON file, which contains an array of documented entities,
            process the data, and add it to our database. '''
        with open(filename) as fd:
            data = json.load(fd)

        def add_this_and_children(jsondata):
            number = self._add_or_merge_node(jsondata)
            # Replace the children raw nodes with an actual number
            if 'members' in jsondata:
                jsondata['members'] = [add_this_and_children(sub)
                                       for sub in jsondata['members']]
            return number

        # For all nodes in the input, add them
        for node in data:
            add_this_and_children(node)

    def _add_or_merge_node(self, jsonnode):
        ''' Add the JSON documentation node to our list of known entities,
            potentially merging it with other nodes in the process. This will
            return the number of the node in the allEntities array. '''

        nextnum = len(self.allEntities)
        if 'qualname' in jsonnode:
            # This has a qualifying name within the TU that we can use to merge.
            # Get it and do so.
            qname = jsonnode['qualname']
            if qname in self.qualmap:
                return self.qualmap[qname]

            # No one else to merge -> add it.
            self.qualmap[qname] = nextnum
        self.allEntities.append(jsonnode)
        return nextnum

    def _make_file_nodes(self):
        filemap = dict()
        for num in xrange(len(self.allEntities)):
            node = self.allEntities[num]
            if 'location' not in node:
                continue
            filename = node['location'].split(':')[0]
            if filename in filemap:
                filemap[filename]['members'].append(num)
            else:
                filenode = {
                    'doctype': 'file',
                    'briefdoc': '',
                    'fulldoc': '',
                    'location': filename,
                    'members': [num]
                    }
                filemap[filename] = filenode
                self.allEntities.append(filenode)

    def _clean_raw_node(self, node):
        ''' Process the raw node to clean it up for final presentation view.
            This will do things like fix documentation links, remove extra data,
            and other similar nodes. '''

        # Convert documentation as necessary
        for key in ['fulldoc', 'briefdoc']:
            if key in node:
                node[key] = self.processDocumentation(node[key])

        # Fix locations to be correct
        if 'location' in node:
            parts = node['location'].split(':')
            parts[0] = os.path.relpath(os.path.realpath(parts[0]),
                                       self.tree.source_folder)
            node['location'] = ':'.join(parts)

    def _remove_nodes(self):
        ''' Filter out unnecessary entities from the documentation index. '''
        newList = list()
        adjustMap = dict()
        self.qualmap = dict()
        for num in xrange(len(self.allEntities)):
            entity = self.allEntities[num]
            # We earlier processed the locations to all be relative to the
            # source directory. If a path is outside, then this node is not
            # useful for our documentation purposes and thus needs to go
            # away. Mark it for removal in all subgroups
            if entity['location'].startswith('..'):
                adjustMap[num] = None
            else:
                # len(toRemove) nodes prior to this one will be deleted, so we
                # need to adjust the numbers of successors
                adjustMap[num] = len(newList)
                if 'qualname' in entity:
                    self.qualmap[entity['qualname']] = len(newList)
                newList.append(entity)

        # We have our new list. Now go through the list and adjust the numbers.
        for node in newList:
            if 'members' in node:
                node['members'] = filter(lambda x : x != None,
                    (adjustMap[x] for x in node['members']))

        # Update the list of all nodes now
        self.allEntities = newList

    def _resolve_links(self):
        ''' Replace {foo} constructs in code variables with proper HTML-tagged
            output. '''
        for node in self.allEntities:
            for prop in ['prename', 'postname', 'prolog']:
                if prop in node:
                    node[prop] = self._resolve_raw_string(node[prop])

    def _resolve_raw_string(self, string):
        ''' Inner helper for _resolve_links. '''
        # Find all qualified names
        if not hasattr(self, 'synsplit'):
            self.synsplit = re.compile('({[^}]*})')
        parts = self.synsplit.split(string)

        def sanitize_part(part):
            if len(part) > 0 and part[0] == '{':
                qualname = part[1:-1]
                pretty = escape_html(qualname)
                if qualname in self.qualmap:
                    docid = self.qualmap[qualname]
                    refed = self.allEntities[docid]
                    pretty = '<a href="%s/%s/docs/%d">%s</a>' % (
                        self.tree.config.wwwroot, self.tree.name, docid,
                        escape_html(refed['shortname']))
                return pretty
            else:
                return escape_html(part)
        
        return ''.join(sanitize_part(p) for p in parts)

    def processDocumentation(self, doc):
        lines = doc.split('\n')
        outbuffer = ''
        inPara = False
        for line in lines:
            trimmed = line.strip()
            if inPara and trimmed == "":
                outbuffer += "</p>"
                inPara = False
                continue
            elif not inPara and trimmed != "":
                outbuffer += "<p>"
                inPara = True
            if trimmed != "":
                outbuffer += trimmed + " "
        if inPara:
            outbuffer += "</p>"
        return outbuffer

    def output_to_database(self, conn):
        ''' Completes post-processing of the documentation data and outputs the
            final results to the DXR database. '''
        # Clean up all of the raw nodes
        for node in self.allEntities:
            self._clean_raw_node(node)
        self._remove_nodes()
        self._resolve_links()
        self._make_file_nodes()

        # Create the table in the database.
        conn.executescript("""CREATE TABLE IF NOT EXISTS documentation (
                               docid INTEGER NOT NULL PRIMARY KEY,
                               docstart INTEGER NOT NULL,
                               doclen INTEGER NOT NULL,
                               kind VARCHAR(20) NOT NULL);""")

        # Write the output to a separate file to avoid cluttering the database.
        # Note that this is a json file, but we can't just json.dump on all of
        # our entities, since we use the database to get a specific byte range
        # of the file.
        docsout = os.path.join(self.tree.target_folder, ".dxr-docs.json")
        needsComma = False
        with open(docsout, 'w') as outfd:
            for num in xrange(len(self.allEntities)):
                outfd.write(',\n' if needsComma else '[')
                needsComma = True
                start = outfd.tell()
                json.dump(self.allEntities[num], outfd)
                conn.execute("""INSERT INTO documentation
                                (docid, docstart, doclen, kind)
                                VALUES (%d, %d, %d, ?)""" %
                             (num, start, outfd.tell() - start),
                             (self.allEntities[num]['doctype'],))
            outfd.write(']')

# Loaders for lazy database loading
loaders = {
  "file": FileEntity,
  "namespace": NamespaceEntity,
  "aggregate": AggregateEntity,
  "leaf": LeafEntity
}

class LazyDocumentationLoader(object):
  ''' A class which lets documentation be loaded lazily from the database. '''
  def __init__(self, conn, docfile):
    self.conn = conn
    self.docfile = open(docfile, 'r')

  def __del__(self):
    self.docfile.close()

  def getEntity(self, number):
    row = self.conn.execute(
        "SELECT * FROM documentation WHERE docid=%d" % number).fetchone()
    if row is None:
      return None
    self.docfile.seek(row['docstart'])
    data = json.loads(self.docfile.read(row['doclen']))
    return loaders[data["doctype"]](data, number, self)

def getDocumentedEntity(conn, instance_path, tree, num):
    lazyloader = LazyDocumentationLoader(conn,
        os.path.join(instance_path, 'trees', tree, '.dxr-docs.json'))
    return lazyloader.getEntity(num)

global_lists = [
    ("Namespaces", "namespace"),
    ("Classes", "aggregate")
]
def get_global_lists(conn, instance_path, tree):
    return list(get_global_lists_inner(conn, instance_path, tree))
def get_global_lists_inner(conn, instance_path, tree):
    lazyloader = LazyDocumentationLoader(conn,
        os.path.join(instance_path, 'trees', tree, '.dxr-docs.json'))
    for pretty, kind in global_lists:
      print kind
      yield (pretty, (lazyloader.getEntity(n['docid']) for n in conn.execute(
                      "SELECT docid FROM documentation WHERE kind=?", (kind,))
                      .fetchall()))
